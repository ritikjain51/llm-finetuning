{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritikjain51/llm-finetuning/blob/main/Fine_Tuning_LLAMA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7rrkNOmfApl",
        "outputId": "6bfa997d-215e-4981-a94c-d2eef45e0285"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 24 06:10:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luppSKpVeMlE"
      },
      "outputs": [],
      "source": [
        "pip install accelerate peft bitsandbytes transformers trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v709KmHreMlF"
      },
      "source": [
        "## Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y2sJv_T_eMlF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline, logging\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y42KQtINeMlF"
      },
      "source": [
        "Llama2 uses the Chat Model\n",
        "\n",
        "Prompt Template\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "System Prompt\n",
        "<<SYS>>\n",
        "\n",
        "User prompt [/INST] Model Answer </s>\n",
        "\n",
        "```\n",
        "\n",
        "Sample Prompt\n",
        "\n",
        "```\n",
        "<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgwJ7-smeMlG"
      },
      "source": [
        "### Convert Dataset to LLAMA2 Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r6fveB3eMlG",
        "outputId": "58872645-be8c-4464-eac1-66e9d410cfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset('timdettmers/openassistant-guanaco')\n",
        "\n",
        "# Shuffle the dataset and slice it\n",
        "dataset = dataset['train'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# Define a function to transform the data\n",
        "def transform_conversation(example):\n",
        "    conversation_text = example['text']\n",
        "    segments = conversation_text.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over pairs of segments\n",
        "    for i in range(1, len(segments) - 1, 2):\n",
        "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
        "\n",
        "        # Check if there is a corresponding assistant segment before processing\n",
        "        if i + 1 < len(segments):\n",
        "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
        "\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_dataset = dataset.map(transform_conversation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcfn09EbeMlG",
        "outputId": "0fcb1159-4b15-44ef-83c0-89af8cfbfe3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>\n"
          ]
        }
      ],
      "source": [
        "print(transformed_dataset.data[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvUzNKy6eMlG"
      },
      "source": [
        "### Intiaizing Parameters\n",
        "\n",
        "- Model Parameters\n",
        "- Model Quantization (BitsAndBytes)\n",
        "- PEFT Parameters (LoRA)\n",
        "- Training Parameters\n",
        "- Trainer Paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9DeZfuzneMlG"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "new_model_name = \"Llama-2-chat-finetuned\"\n",
        "\n",
        "\n",
        "############################\n",
        "# QLoRA Parameters\n",
        "# LoRA Attention Dimension (Rank)\n",
        "lora_r = 64\n",
        "\n",
        "# LoRA scaling factor\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout Probability\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "\n",
        "###################################\n",
        "# BitsAndBytes Parameters\n",
        "\n",
        "# Activate 4-bit precisoin for base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4, nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base model\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "LM67bsk1eMlG"
      },
      "outputs": [],
      "source": [
        "############ Training Arguments ################\n",
        "# Output Directory\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of Training Epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for Training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size for Evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update Steps to accumulate gredient\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gredient checkpointing\n",
        "gredient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (Gradient Clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning Rate (AdamW Optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight Decay to apply on all layers\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning Rate Schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequence into batches with same length\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updated steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X step\n",
        "logging_steps=25\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qQp0Y8QeeMlG"
      },
      "outputs": [],
      "source": [
        "############# SFT Parameters ###################\n",
        "### Supervised Fine-tuning\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = True\n",
        "\n",
        "# Pack multiple short examples in same input sequence\n",
        "packing = False\n",
        "\n",
        "# Load the entire model\n",
        "device_map = {\"\": 0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8G2OAe2eMlG"
      },
      "source": [
        "## Load the model, dataset and Start Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9AJPon7neMlG"
      },
      "outputs": [],
      "source": [
        "## Load the Dataset\n",
        "dataset = transformed_dataset\n",
        "# dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "\n",
        "# Load Tokenizer and model with QLoRA Configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = use_4bit,\n",
        "    bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype = compute_dtype,\n",
        "    bnb_4bit_use_double_quant = use_nested_quant\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj9gavAmeMlG",
        "outputId": "0bf76dda-77f1-4076-ccda-74e1c36d2034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BitsAndBytesConfig {\n",
              "  \"_load_in_4bit\": true,\n",
              "  \"_load_in_8bit\": false,\n",
              "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
              "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "  \"bnb_4bit_quant_type\": \"nf4\",\n",
              "  \"bnb_4bit_use_double_quant\": false,\n",
              "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "  \"llm_int8_has_fp16_weight\": false,\n",
              "  \"llm_int8_skip_modules\": null,\n",
              "  \"llm_int8_threshold\": 6.0,\n",
              "  \"load_in_4bit\": true,\n",
              "  \"load_in_8bit\": false,\n",
              "  \"quant_method\": \"bitsandbytes\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "bnb_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GWpmaXOFeMlH"
      },
      "outputs": [],
      "source": [
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "063a2830c4f0475e8fe01df1c90696e5",
            "196e404c671a4b6caa71f881917f1622",
            "b7a9903c80d04fa78c77f4cf3aad3b4c",
            "35b63672780d4564b6120910b98ffd2e",
            "2e49b0730b4b4209915fa8bb7e11cef3",
            "9e92794e179d42cc9d346ca8e478a9df",
            "eb225743b47d4859b7ab4bc57b7850af",
            "abe615788b774306bf4f86c84ca82618",
            "c9d6150d999b46a395d5d2275bb20bf1",
            "8447c14868d9437a94a06252c5f23953",
            "7d819d9f48fb45fcb2a63e254f6638f1"
          ]
        },
        "id": "OrmGG1DYeMlH",
        "outputId": "97cd36a2-a65d-4cde-a468-d1da8cda6453"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "063a2830c4f0475e8fe01df1c90696e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8IG1Q1c6eMlH"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load LoRA Configuration\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    r = lora_r,\n",
        "    bias = \"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgHIrPcwfl74",
        "outputId": "6f693b48-7075-42e1-af2a-639f8f271df4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    per_device_train_batch_size = per_device_train_batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    optim = optim,\n",
        "    save_steps = save_steps,\n",
        "    logging_steps = logging_steps,\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay = weight_decay,\n",
        "    fp16 = fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm = max_grad_norm,\n",
        "    max_steps = max_steps,\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    group_by_length = group_by_length,\n",
        "    lr_scheduler_type = lr_scheduler_type,\n",
        "    report_to = \"tensorboard\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "5dHRjzH1ghaQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trainer Parameters\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    peft_config = peft_config,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_arguments,\n",
        "    packing=packing\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1723ee442efd46efa0a62a6c02def5e0",
            "81098bcaea584214846a668383cd76e3",
            "ec7cb268cf5d45b89fb2c82ef4c0b254",
            "693961129d7d43149cb2f66615577b9f",
            "97e5b89dd3394d4cb5da8a5953a6bab3",
            "1487df65b64e43a6b65aff8aa2b8a36e",
            "cab842471fa84c77bb208bd4c5bae31f",
            "17817e06a7434eefa29edd191bbb0874",
            "65bcbe6b4aca4d5e85a577a39fd5142d",
            "017413a5c307469ba693a6e7ea8dcd3e",
            "b07c3e5300fd42c58ca209a5733d6c35"
          ]
        },
        "id": "ZNffrtcRPZn3",
        "outputId": "69c993cc-8a1f-4428-a09f-77be10cd5514"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1723ee442efd46efa0a62a6c02def5e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train model\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "p1_dtQLgP4oB",
        "outputId": "2f9398a8-6a14-4daa-ce71-8879cb225874"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 03:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=0.0, metrics={'train_runtime': 224.3644, 'train_samples_per_second': 4.457, 'train_steps_per_second': 1.114, 'total_flos': 39845388288000.0, 'train_loss': 0.0, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(new_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3BFBVlsP_Mz",
        "outputId": "22cd7908-2051-419c-c3c5-e4c011140a22"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "qbdkmoH7RCQs",
        "outputId": "b382c474-a923-417f-a1fe-ccf4b81c55f0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference using pipeline"
      ],
      "metadata": {
        "id": "pussQFDabDVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How large language models can help humans?\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,\n",
        "                max_length=400)\n",
        "\n",
        "prompt_template = \"[INST]{prompt}[\\INST]\"\n",
        "\n",
        "result = pipe(prompt_template.format(prompt = prompt))\n",
        "\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07AqneoqVOR-",
        "outputId": "039a859a-a7fe-4bea-e182-e03d2ec963e6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]How large language models can help humans?[\\INST]  Large language models, such as transformer-based models like BERT, RoBERTa, and XLNet, have revolutionized the field of natural language processing (NLP) in recent years. everybody is talking about them, and they have achieved state-of-the-art results on a wide range of NLP tasks. But how do these models actually help humans? Here are some ways in which large language models can help humans:\n",
            "\n",
            "1. Improved language understanding: Large language models can help humans by improving our understanding of language. By learning to predict the next word in a sequence of text, these models can help us better understand the context and meaning of a sentence or paragraph.\n",
            "2. Language translation: Large language models can be trained on large datasets of text in multiple languages, allowing them to learn to translate between languages. This can help humans by enabling us to communicate with people who speak different languages.\n",
            "3. Text generation: Large language models can generate text that is coherent and natural-sounding, allowing humans to create content such as articles, blog posts, and social media updates.\n",
            "4. Chatbots and virtual assistants: Large language models can be used to build chatbots and virtual assistants that can understand and respond to user queries in a natural and human-like way.\n",
            "5. Sentiment analysis: Large language models can be used to analyze the sentiment of text, allowing humans to understand the emotional tone of a piece of text and make decisions based on that information.\n",
            "6. Named entity recognition: Large language models can be used to identify and classify named entities in text, such as people, organizations, and locations.\n",
            "7. Question answering: Large language models can be used to answer questions based on the information contained in a piece of text,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "\n",
        "del model\n",
        "del trainer\n",
        "del tokenizer\n",
        "del pipe\n",
        "del dataset\n",
        "del transformed_dataset\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95wZm2DfcEfP",
        "outputId": "91405fc1-b5b0-4879-ac2e-5438b3298abc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing New Llama2 Model"
      ],
      "metadata": {
        "id": "yxqIMBNadXwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map = device_map\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "62cff5505b3b4000b9be282b2b738b22",
            "7f41c45a8d0d4342aab8117971073125",
            "90d5d3e900ab4bc68fb854be537baacc",
            "c4636b5609d0447d98f1fdb08fca5fe1",
            "69ff261ade09465784889e1cb9ed3829",
            "aeca4465906a4bccb6b4993e7336c605",
            "1c8424206ac14f87ad65d4818ccdcd22",
            "8c3741c2be87456c82284f6103e57ebc",
            "637d44ea51954dd59a2da72ed0a0c5fd",
            "6528981759d74b0087d4b2ebe8f93ec4",
            "ccc1b79dbbb449458fc38a6fa6ef3a45"
          ]
        },
        "id": "KQJygp0ydB7v",
        "outputId": "229bb66c-1c0b-4be2-8310-30611003ec3e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62cff5505b3b4000b9be282b2b738b22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
        "peft_model = peft_model.merge_and_unload()\n"
      ],
      "metadata": {
        "id": "xzx6Og7VdpUw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_cache=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "uBDjnq8SdzTz"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer, max_length=300)"
      ],
      "metadata": {
        "id": "EdGv3BT-eczz"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(prompt_template.format(prompt=prompt))\n",
        "\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxU9CoAzer5_",
        "outputId": "344a4ad8-51ab-40f1-c52f-c541e3282389"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]How large language models can help humans?[\\INST]  Large language models, such as transformer-based models like BERT, RoBERTa, and XLNet, have revolutionized the field of natural language processing (NLP) in recent years. Here are some ways in which large language models can help humans:\n",
            "\n",
            "1. Improved language understanding: Large language models can analyze and understand language in ways that were previously impossible. They can recognize subtle nuances in language, such as the context and tone of a sentence, and use this understanding to improve language translation, question answering, and other NLP tasks.\n",
            "2. Enhanced language generation: Large language models can generate high-quality text that is coherent and natural-sounding. This can be useful for a variety of applications, such as chatbots, language translation, and content generation.\n",
            "3. Better text summarization: Large language models can summarize long documents or articles into shorter summaries that capture the most important information. This can save time and improve comprehension for readers.\n",
            "4. Improved sentiment analysis: Large language models can analyze text to determine the sentiment or emotional tone behind it. This can be useful for applications such as customer service, where understanding the sentiment of customer feedback is important.\n",
            "5. Enhanced dialogue systems: Large language models can be used to build more sophisticated dialog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OweWL4qseyOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "063a2830c4f0475e8fe01df1c90696e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_196e404c671a4b6caa71f881917f1622",
              "IPY_MODEL_b7a9903c80d04fa78c77f4cf3aad3b4c",
              "IPY_MODEL_35b63672780d4564b6120910b98ffd2e"
            ],
            "layout": "IPY_MODEL_2e49b0730b4b4209915fa8bb7e11cef3"
          }
        },
        "196e404c671a4b6caa71f881917f1622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e92794e179d42cc9d346ca8e478a9df",
            "placeholder": "​",
            "style": "IPY_MODEL_eb225743b47d4859b7ab4bc57b7850af",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b7a9903c80d04fa78c77f4cf3aad3b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abe615788b774306bf4f86c84ca82618",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9d6150d999b46a395d5d2275bb20bf1",
            "value": 2
          }
        },
        "35b63672780d4564b6120910b98ffd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8447c14868d9437a94a06252c5f23953",
            "placeholder": "​",
            "style": "IPY_MODEL_7d819d9f48fb45fcb2a63e254f6638f1",
            "value": " 2/2 [00:54&lt;00:00, 25.01s/it]"
          }
        },
        "2e49b0730b4b4209915fa8bb7e11cef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e92794e179d42cc9d346ca8e478a9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb225743b47d4859b7ab4bc57b7850af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abe615788b774306bf4f86c84ca82618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d6150d999b46a395d5d2275bb20bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8447c14868d9437a94a06252c5f23953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d819d9f48fb45fcb2a63e254f6638f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1723ee442efd46efa0a62a6c02def5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81098bcaea584214846a668383cd76e3",
              "IPY_MODEL_ec7cb268cf5d45b89fb2c82ef4c0b254",
              "IPY_MODEL_693961129d7d43149cb2f66615577b9f"
            ],
            "layout": "IPY_MODEL_97e5b89dd3394d4cb5da8a5953a6bab3"
          }
        },
        "81098bcaea584214846a668383cd76e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1487df65b64e43a6b65aff8aa2b8a36e",
            "placeholder": "​",
            "style": "IPY_MODEL_cab842471fa84c77bb208bd4c5bae31f",
            "value": "Map: 100%"
          }
        },
        "ec7cb268cf5d45b89fb2c82ef4c0b254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17817e06a7434eefa29edd191bbb0874",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65bcbe6b4aca4d5e85a577a39fd5142d",
            "value": 1000
          }
        },
        "693961129d7d43149cb2f66615577b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_017413a5c307469ba693a6e7ea8dcd3e",
            "placeholder": "​",
            "style": "IPY_MODEL_b07c3e5300fd42c58ca209a5733d6c35",
            "value": " 1000/1000 [00:02&lt;00:00, 425.17 examples/s]"
          }
        },
        "97e5b89dd3394d4cb5da8a5953a6bab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1487df65b64e43a6b65aff8aa2b8a36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab842471fa84c77bb208bd4c5bae31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17817e06a7434eefa29edd191bbb0874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65bcbe6b4aca4d5e85a577a39fd5142d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "017413a5c307469ba693a6e7ea8dcd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07c3e5300fd42c58ca209a5733d6c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62cff5505b3b4000b9be282b2b738b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f41c45a8d0d4342aab8117971073125",
              "IPY_MODEL_90d5d3e900ab4bc68fb854be537baacc",
              "IPY_MODEL_c4636b5609d0447d98f1fdb08fca5fe1"
            ],
            "layout": "IPY_MODEL_69ff261ade09465784889e1cb9ed3829"
          }
        },
        "7f41c45a8d0d4342aab8117971073125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeca4465906a4bccb6b4993e7336c605",
            "placeholder": "​",
            "style": "IPY_MODEL_1c8424206ac14f87ad65d4818ccdcd22",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "90d5d3e900ab4bc68fb854be537baacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c3741c2be87456c82284f6103e57ebc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_637d44ea51954dd59a2da72ed0a0c5fd",
            "value": 2
          }
        },
        "c4636b5609d0447d98f1fdb08fca5fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6528981759d74b0087d4b2ebe8f93ec4",
            "placeholder": "​",
            "style": "IPY_MODEL_ccc1b79dbbb449458fc38a6fa6ef3a45",
            "value": " 2/2 [01:07&lt;00:00, 31.21s/it]"
          }
        },
        "69ff261ade09465784889e1cb9ed3829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeca4465906a4bccb6b4993e7336c605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c8424206ac14f87ad65d4818ccdcd22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c3741c2be87456c82284f6103e57ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637d44ea51954dd59a2da72ed0a0c5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6528981759d74b0087d4b2ebe8f93ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc1b79dbbb449458fc38a6fa6ef3a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}